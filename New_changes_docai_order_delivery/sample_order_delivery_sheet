CREATE OR REPLACE TABLE DS_DEV_DB.DOC_AI_SCHEMA.MODEL_METADATA (
    MODEL_NAME VARCHAR(255),          
    FLATTEN_TABLE VARCHAR(255),       
    VALIDATED_TABLE VARCHAR(255),     
    FOLDER_NAME VARCHAR(255),         
    PREDICTION_TYPE NUMBER(1)        
);

select * from MODEL_METADATA;


INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.MODEL_METADATA (MODEL_NAME, FLATTEN_TABLE, VALIDATED_TABLE, FOLDER_NAME, PREDICTION_TYPE)
VALUES
    ('FACTORY_ORDER', 'Order_Flatten', 'Order_Validated', 'Order', 2),
    ('FACTORY_DELIVERY', 'Delivery_Flatten', 'Delivery_Validated', 'Delivery', 2);

create or replace TABLE DS_DEV_DB.DOC_AI_SCHEMA.DOCAI_PREFILTER (
	ROWID NUMBER(38,0) autoincrement start 1 increment 1 noorder,
	MODEL_NAME VARCHAR(255),
	FILENAME VARCHAR(16777216),
	FILESIZE VARCHAR(255),
	NUMBER_OF_PAGES NUMBER(38,0),
	DATECREATED TIMESTAMP_LTZ(9),
	COMMENT VARCHAR(16777216),
	STATUS VARCHAR(16777216),
	PROCESS_START_TIME TIMESTAMP_LTZ(9),
	PROCESS_END_TIME TIMESTAMP_LTZ(9)
);


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.COUNT_PDF_PAGES_PROC()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.11'
PACKAGES = ('snowflake-snowpark-python', 'PyPDF2')
HANDLER = 'count_pages'
EXECUTE AS OWNER
AS '
import snowflake.snowpark as snowpark
import PyPDF2
import os
from datetime import datetime

def count_pages(session):
    # Temporary directory for downloading files
    temp_dir = "/tmp/pdf_files/"
    os.makedirs(temp_dir, exist_ok=True)

    # Stream and table details
    stream_name = "DS_DEV_DB.DOC_AI_SCHEMA.PREPROCESS_STREAM"
    temp_table_name = "STREAMDATA_TEMP"
    prefilter_table_name = "DS_DEV_DB.DOC_AI_SCHEMA.DOCAI_PREFILTER"
    metadata_table_name = "DS_DEV_DB.DOC_AI_SCHEMA.MODEL_METADATA"

    # Step 1: Create a temporary table to store stream data
    session.sql(f"""
        CREATE OR REPLACE TABLE {temp_table_name} AS
        SELECT * FROM {stream_name} WHERE METADATA$ACTION = ''INSERT'';
    """).collect()

    # Step 2: Fetch data from the temporary table
    temp_data = session.sql(f"SELECT * FROM {temp_table_name}").collect()
    if not temp_data:
        session.sql(f"DROP TABLE IF EXISTS {temp_table_name}").collect()
        return "No files to process in the stream."

    processed_count = 0

    for row in temp_data:
        relative_path = row["RELATIVE_PATH"]  # Extract full relative path (e.g., "Order/2N840077-001.pdf")
        file_size = row["SIZE"]
        stage_file_path = f"@DOC_STAGE/{relative_path}"
        process_start_time = datetime.now()

        # Step 3: Determine MODEL_NAME dynamically based on folder
        folder_name = relative_path.split("/")[0]  # Get the subfolder name (e.g., "Order" or "Delivery")

        metadata_query = f"""
            SELECT MODEL_NAME 
            FROM {metadata_table_name}
            WHERE FOLDER_NAME = ''{folder_name}''
            LIMIT 1
        """
        metadata_result = session.sql(metadata_query).collect()

        # Fallback to UNKNOWN if no match is found
        if metadata_result:
            model_name = metadata_result[0]["MODEL_NAME"]
        else:
            model_name = "UNKNOWN"
            session.sql(f"""
                INSERT INTO {prefilter_table_name} (
                    MODEL_NAME, FILENAME, FILESIZE, NUMBER_OF_PAGES, DATECREATED, COMMENT, STATUS, PROCESS_START_TIME, PROCESS_END_TIME
                ) VALUES (
                    ''UNKNOWN'', ''{relative_path}'', {file_size}, NULL, CURRENT_TIMESTAMP,
                    ''File skipped: Unknown folder type.'', ''SKIPPED'', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                )
            """).collect()
            continue  # Skip to the next file

        # Step 4: Skip files with size 0 bytes
        if file_size == 0:
            session.sql(f"""
                INSERT INTO {prefilter_table_name} (
                    MODEL_NAME, FILENAME, FILESIZE, NUMBER_OF_PAGES, DATECREATED, COMMENT, STATUS, PROCESS_START_TIME, PROCESS_START_TIME
                ) VALUES (
                    ''{model_name}'', ''{relative_path}'', {file_size}, NULL, CURRENT_TIMESTAMP,
                    ''File size is 0 bytes.'', ''SKIPPED'', CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                )
            """).collect()
            continue

        try:
            # Step 5: Insert or update prefilter table for processing
            session.sql(f"""
                INSERT INTO {prefilter_table_name} (
                    MODEL_NAME, FILENAME, FILESIZE, NUMBER_OF_PAGES, DATECREATED, COMMENT, STATUS, PROCESS_START_TIME
                ) SELECT 
                    ''{model_name}'', ''{relative_path}'', {file_size}, NULL, CURRENT_TIMESTAMP,
                    ''Validated Successfully'', ''NOT PROCESSED'', CURRENT_TIMESTAMP
                WHERE NOT EXISTS (
                    SELECT 1 FROM {prefilter_table_name} WHERE FILENAME = ''{relative_path}''
                )
            """).collect()

            # Step 6: Download the file from the stage and count pages
            session.file.get(stage_file_path, temp_dir)
            local_file_path = os.path.join(temp_dir, relative_path.split("/")[-1])

            if not os.path.exists(local_file_path):
                raise FileNotFoundError(f"File {relative_path} not found.")

            with open(local_file_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)
                total_pages = len(pdf_reader.pages)

            # Update the prefilter table with page count
            session.sql(f"""
                UPDATE {prefilter_table_name}
                SET 
                    NUMBER_OF_PAGES = {total_pages},
                    STATUS = ''NOT PROCESSED'',
                    COMMENT = ''Page count updated successfully.'',
                    PROCESS_END_TIME = CURRENT_TIMESTAMP
                WHERE FILENAME = ''{relative_path}''
            """).collect()

        except Exception as e:
            # Handle errors during file processing
            session.sql(f"""
                UPDATE {prefilter_table_name}
                SET STATUS = ''ERROR'',
                    COMMENT = ''Processing failed: {str(e)}''
                WHERE FILENAME = ''{relative_path}''
            """).collect()
        
        finally:
            # Clean up the local file
            if os.path.exists(local_file_path):
                os.remove(local_file_path)

        processed_count += 1

    # Step 7: Drop the temporary table
    session.sql(f"DROP TABLE IF EXISTS {temp_table_name}").collect()
    return f"Processed a total of {processed_count} files successfully."
';

CALL COUNT_PDF_PAGES_PROC();

select * from PREPROCESS_STREAM;

SELECT * FROM DOCAI_PREFILTER;

----------------------------------------------------------------------------------------------

create or replace TABLE DS_DEV_DB.DOC_AI_SCHEMA.DOCAI_ORDERFORM_EXTRACTION (
	RELATIVEPATH VARCHAR(16777216),
	MODEL_NAME VARCHAR(255),
	SIZE NUMBER(38,0),
	FILE_URL VARCHAR(16777216),
	JSON VARIANT,
	COMMENTS VARCHAR(16777216),
	STATUS VARCHAR(16777216),
	PROCESS_START_TIME TIMESTAMP_LTZ(9),
	PROCESS_END_TIME TIMESTAMP_LTZ(9)
);

CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.HANDLE_PDF_FILES()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.11'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
EXECUTE AS OWNER
AS '
from snowflake.snowpark.session import Session
from datetime import datetime

def main(session: Session) -> str:
    try:
        # Query to fetch all files with `NOT PROCESSED` status
        get_file_name_query = """
            SELECT 
                FILENAME, 
                ROWID, 
                CAST(FILESIZE AS FLOAT) AS FILESIZE, 
                CAST(NUMBER_OF_PAGES AS INT) AS NUMBER_OF_PAGES
            FROM DOCAI_PREFILTER
            WHERE STATUS = ''NOT PROCESSED''
        """
        
        # Execute the query
        rows = session.sql(get_file_name_query).collect()
        
        # Check if there are records to process
        if not rows:
            return "No files to process."

        for row in rows:
            file_name = row["FILENAME"]  # Full relative path (e.g., "Order/2N840077-001.pdf")
            row_id = row["ROWID"]
            file_size_mb = row["FILESIZE"] / (1024 * 1024)  # Convert bytes to MB
            number_of_pages = row["NUMBER_OF_PAGES"]

            try:
                # Extract folder name from the full file path
                folder_name = file_name.split("/")[0]  # Get folder (e.g., "Order" or "Delivery")
                
                # Query metadata table to determine model and prediction type
                metadata_query = f"""
                    SELECT MODEL_NAME, PREDICTION_TYPE
                    FROM DS_DEV_DB.DOC_AI_SCHEMA.MODEL_METADATA
                    WHERE FOLDER_NAME = ''{folder_name}''
                    LIMIT 1
                """
                metadata_result = session.sql(metadata_query).collect()

                # If no matching metadata found, skip the file
                if not metadata_result:
                    session.sql(f"""
                        UPDATE DOCAI_PREFILTER
                        SET STATUS = ''SKIPPED'',
                            COMMENT = ''No matching metadata for folder {folder_name}.'',
                            PROCESS_END_TIME = CURRENT_TIMESTAMP
                        WHERE ROWID = ''{row_id}''
                    """).collect()
                    continue

                # Extract model details
                model_name = metadata_result[0]["MODEL_NAME"]
                prediction_type = metadata_result[0]["PREDICTION_TYPE"]

                # Check the file size and number of pages criteria
                if file_size_mb > 1 and number_of_pages > 5:
                    # Move files to manual_review stage
                    session.sql(f"""
                        COPY FILES INTO @MANUAL_REVIEW
                        FROM @DOC_STAGE
                        FILES = (''{file_name}'')
                    """).collect()

                    # Update status to MANUAL REVIEW
                    session.sql(f"""
                        UPDATE DOCAI_PREFILTER
                        SET STATUS = ''MANUAL REVIEW'',
                            COMMENT = ''File moved to manual_review stage as it failed both criteria.'',
                            PROCESS_END_TIME = CURRENT_TIMESTAMP
                        WHERE ROWID = ''{row_id}''
                    """).collect()
                    continue  # Skip to the next file in the loop

                # Update status to IN PROGRESS
                session.sql(f"""
                    UPDATE DOCAI_PREFILTER
                    SET STATUS = ''IN PROGRESS'',
                        COMMENT = ''Processing in progress.''
                    WHERE ROWID = ''{row_id}''
                """).collect()

                # Record the start time for processing
                process_start_time = session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]

                # Process files meeting criteria
                json_result = session.sql(f"""
                    SELECT DS_DEV_DB.DOC_AI_SCHEMA.{model_name}!PREDICT(
                        GET_PRESIGNED_URL(@DOC_STAGE, ''{file_name}''), {prediction_type}
                    )
                """).collect()[0][0]

                # Insert into extraction table
                session.sql(f"""
                    INSERT INTO DocAI_OrderForm_Extraction (
                        RELATIVEPATH, Model_Name, Size, File_Url, JSON, Comments, Status, PROCESS_START_TIME, PROCESS_END_TIME
                    )
                    SELECT 
                        ''{file_name}'' AS FILENAME, 
                        ''{model_name}'' AS Model_Name,
                        {row["FILESIZE"]} AS Size, 
                        GET_PRESIGNED_URL(@DOC_STAGE, ''{file_name}'') AS File_Url, 
                        PARSE_JSON(''{json_result}'') AS JSON,
                        ''File processed successfully.'' AS Comments,
                        ''NOT PROCESSED'' AS Status,
                        ''{process_start_time}'' AS PROCESS_START_TIME,
                        CURRENT_TIMESTAMP AS PROCESS_END_TIME
                """).collect()

                # Update status to PROCESSED
                session.sql(f"""
                    UPDATE DOCAI_PREFILTER
                    SET STATUS = ''PROCESSED'',
                        COMMENT = ''File processed and moved to DocAI_OrderForm_Extraction.''
                    WHERE ROWID = ''{row_id}''
                """).collect()

            except Exception as e:
                # Log error in extraction table
                session.sql(f"""
                    UPDATE DOCAI_PREFILTER
                    SET STATUS = ''ERROR'',
                        COMMENT = ''Error during processing: {str(e)}''
                    WHERE ROWID = ''{row_id}''
                """).collect()

        return "Files processed successfully."

    except Exception as e:
        return f"General Error: {str(e)}"
';


CALL DS_DEV_DB.DOC_AI_SCHEMA.HANDLE_PDF_FILES();

select * from DOCAI_PREFILTER;

select * from DOCAI_ORDERFORM_EXTRACTION;

update DOCAI_PREFILTER set STATUS ='MANUAL REVIEW' where FILENAME = 'Delivery/2N920027-003.pdf';

update DOCAI_PREFILTER set STATUS ='NOT PROCESSED' where STATUS = 'IN PROGRESS';

-----------------------------------------------------------------------------------------------

CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.MANAGE_MANUAL_REVIEW_FILES()

RETURNS VARCHAR(16777216)

LANGUAGE JAVASCRIPT

EXECUTE AS CALLER

AS '

try {

    // Create temporary table for file tracking

    var create_temp_table = `

        CREATE OR REPLACE TEMPORARY TABLE temp_files_to_move AS

        SELECT FILENAME

        FROM DOCAI_PREFILTER

        WHERE STATUS = ''MANUAL REVIEW''`;

    snowflake.execute({ sqlText: create_temp_table });

    // Check if there are any files to process

    var check_files = `SELECT COUNT(*) AS file_count FROM temp_files_to_move`;

    var files_result = snowflake.execute({ sqlText: check_files });

    files_result.next();

    var file_count = files_result.getColumnValue(1);

    if (file_count === 0) {

        return "No files found with MANUAL_REVIEW status";

    }

    // Get the pattern string for file operations

    var get_pattern = `SELECT LISTAGG(FILENAME, ''|'') FROM temp_files_to_move`;

    var pattern_result = snowflake.execute({ sqlText: get_pattern });

    pattern_result.next();

    var file_pattern = pattern_result.getColumnValue(1);

    // Create the complete pattern string in JavaScript

    var complete_pattern = ''.*('' + file_pattern + '').*'';

    // Copy files to Manual_Review stage

    var copy_files = `COPY FILES INTO @Manual_Review 

                      FROM @DOC_STAGE

                      PATTERN = ''${complete_pattern}''`;

    snowflake.execute({ sqlText: copy_files });

    // Remove files from source stage

    var remove_files = `REMOVE @DOC_STAGE 

                       PATTERN = ''${complete_pattern}''`;

    snowflake.execute({ sqlText: remove_files });

    // Update status to Failed

    var update_status = `UPDATE DOCAI_PREFILTER

                        SET STATUS = ''FAILED''

                        WHERE STATUS = ''MANUAL REVIEW''

                        AND FILENAME IN (SELECT FILENAME FROM temp_files_to_move)`;

    var update_result = snowflake.execute({ sqlText: update_status });

    // Clean up temporary table

    var cleanup = `DROP TABLE IF EXISTS temp_files_to_move`;

    snowflake.execute({ sqlText: cleanup });

    return `Successfully processed ${file_count} files`;

} catch (err) {

    // Clean up temporary table in case of error

    try {

        snowflake.execute({ sqlText: `DROP TABLE IF EXISTS temp_files_to_move` });

    } catch (cleanup_err) {

        // Ignore cleanup errors

    }

    return `Failed to process files: ${err}`;

}

';


CALL MANAGE_MANUAL_REVIEW_FILES();

select * from DOCAI_PREFILTER;

----------------------------------------------------------



CREATE OR REPLACE TABLE DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD (
    MODEL_NAME VARCHAR(255),         
    SCORE_NAME VARCHAR(255),         
    SCORE_VALUE FLOAT              
);

select * from SCORE_THRESHOLD;


INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD (MODEL_NAME, SCORE_NAME, SCORE_VALUE)
VALUES 
    ('FACTORY_ORDER', 'ocrScore', 0.9),
    ('FACTORY_ORDER', 'Altitude', 0.95),
    ('FACTORY_ORDER', 'AmbientTemp', 0.95),
    ('FACTORY_ORDER', 'Capacity', 0.95),
    ('FACTORY_ORDER', 'Contract_Order_No', 0.95),
    ('FACTORY_ORDER', 'Contract_Order_Number', 0.85),
    ('FACTORY_ORDER', 'EWT', 0.95),
    ('FACTORY_ORDER', 'EWT_F', 0.2),
    ('FACTORY_ORDER', 'Fabrication_Status', 0.95),
    ('FACTORY_ORDER', 'FF', 0.95),
    ('FACTORY_ORDER', 'Flow_Score', 0.95),
    ('FACTORY_ORDER', 'Fluid', 0.85),
    ('FACTORY_ORDER', 'Liquidated_Damages', 0.95),
    ('FACTORY_ORDER', 'Liquidated_Damages_Comments', 0.95),
    ('FACTORY_ORDER', 'LWT', 0.95),
    ('FACTORY_ORDER', 'LWT_F', 0.5),
    ('FACTORY_ORDER', 'MBH', 0.95),
    ('FACTORY_ORDER', 'Model_No', 0.95),
    ('FACTORY_ORDER', 'Order_Details_Description', 0.95),
    ('FACTORY_ORDER', 'Order_Placed_Date', 0.95),
    ('FACTORY_ORDER', 'Order_Rev_No', 0.95),
    ('FACTORY_ORDER', 'PD', 0.95),
    ('FACTORY_ORDER', 'Pass', 0.85),
    ('FACTORY_ORDER', 'PIN', 0.9),
    ('FACTORY_ORDER', 'Power', 0.95),
    ('FACTORY_ORDER', 'Refrigerant', 0.95),
    ('FACTORY_ORDER', 'Revision_Date', 0.95),
    ('FACTORY_ORDER', 'Revision_Number', 0.95),
    ('FACTORY_ORDER', 'Shipping_Weight', 0.95),
    ('FACTORY_ORDER', 'Special_Quotes', 0.9),
    ('FACTORY_ORDER', 'Total_KW', 0.95),
    ('FACTORY_ORDER', 'Type_Starter', 0.95),
    ('FACTORY_ORDER', 'Unit_Tag', 0.95);


INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD (MODEL_NAME, SCORE_NAME, SCORE_VALUE)
VALUES
    ('FACTORY_DELIVERY', 'ocrScore', 0.92),          
    ('FACTORY_DELIVERY', 'ALTITUDE', 0.99),         
    ('FACTORY_DELIVERY', 'AMBIENT_TEMP', 0.97),     
    ('FACTORY_DELIVERY', 'DELIVERY_ADDRESS', 0.55), 
    ('FACTORY_DELIVERY', 'EWT', 0.997),             
    ('FACTORY_DELIVERY', 'EWT_F', 0.94),           
    ('FACTORY_DELIVERY', 'FF', 0.99),               
    ('FACTORY_DELIVERY', 'FLOW', 0.78),             
    ('FACTORY_DELIVERY', 'LWT', 0.90),             
    ('FACTORY_DELIVERY', 'LWT_F', 0.96),           
    ('FACTORY_DELIVERY', 'MBH', 1.00),             
    ('FACTORY_DELIVERY', 'PASS', 1.00),            
    ('FACTORY_DELIVERY', 'PD', 0.65),              
    ('FACTORY_DELIVERY', 'POWER', 0.998),          
    ('FACTORY_DELIVERY', 'SHIPPING_WEIGHT', 1.00), 
    ('FACTORY_DELIVERY', 'TOTAL_KW', 0.75);


-------------------------------------------------------------------------------------

CREATE OR REPLACE TABLE Order_Flatten (
    RELATIVEPATH STRING,
    MODEL_NAME STRING,
    OCR_SCORE FLOAT,
    Contract_Order_Number_Score FLOAT,
    Contract_Order_Number_Value STRING,
    Order_Placed_Date_Score FLOAT,
    Order_Placed_Date_Value STRING,
    Revision_Number_Score FLOAT,
    Revision_Number_Value STRING,
    Liquidated_Damages_Score FLOAT,
    Liquidated_Damages_Value STRING,
    Order_Details_Description_Score FLOAT,
    Order_Details_Description_Value STRING,
    Fabrication_Status_Score FLOAT,
    Fabrication_Status_Value STRING,
    Revision_Date_Score FLOAT,
    Revision_Date_Value STRING,
    Liquidated_Damages_Comment_Score FLOAT,
    Liquidated_Damages_Comment_Value STRING,
    Contract_Order_No_Score FLOAT,
    Contract_Order_No_Value STRING,
    Order_Rev_No_Score FLOAT,
    Order_Rev_No_Value STRING,
    Unit_Tag_Score FLOAT,
    Unit_Tag_Value STRING,
    Model_No_Score FLOAT,
    Model_No_Value STRING,
    Capacity_Score FLOAT,
    Capacity_Value STRING,
    Refrigerant_Score FLOAT,
    Refrigerant_Value STRING,
    PIN_Score FLOAT,
    PIN_Value STRING,
    EWT_Score FLOAT,
    EWT_Value STRING,
    EWT_F_Score FLOAT,
    EWT_F_Value STRING,
    LWT_Score FLOAT,
    LWT_Value STRING,
    LWT_F_Score FLOAT,
    LWT_F_Value STRING,
    Flow_Score FLOAT,
    Flow_Value STRING,
    PD_Score FLOAT,
    PD_Value STRING,
    Fluid_Score FLOAT,
    Fluid_Value STRING,
    FF_Score FLOAT,
    FF_Value STRING,
    MBH_Score FLOAT,
    MBH_Value STRING,
    Pass_Score FLOAT,
    Pass_Value STRING,
    Power_Score FLOAT,
    Power_Value STRING,
    Total_KW_Score FLOAT,
    Total_KW_Value STRING,
    AmbientTemp_Score FLOAT,
    AmbientTemp_Value STRING,
    Altitude_Score FLOAT,
    Altitude_Value STRING,
    Type_Starter_Score FLOAT,
    Type_Starter_Value STRING,
    Shipping_Weight_Score FLOAT,
    Shipping_Weight_Value STRING,
    Special_Quotes_Score FLOAT,
    Special_Quotes_Value STRING,
    COMMENTS STRING,
    STATUS STRING,
    PROCESSED_TIMESTAMP TIMESTAMP,
    PROCESS_START_TIME TIMESTAMP,
    PROCESS_END_TIME TIMESTAMP
);


-- SELECT SCORE_NAME, SCORE_VALUE
-- FROM DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD
-- WHERE MODEL_NAME = 'FACTORY_ORDER';

-- SELECT JSON
-- FROM DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
-- WHERE RELATIVEPATH IN ('Order/0J080807-001.pdf', 'Order/1J080825-001.pdf');

-- DESCRIBE TABLE DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN;


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_FLATTEN()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.11'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
EXECUTE AS OWNER
AS
$$
import json
from snowflake.snowpark.session import Session

class InvoiceProcessor:

    def __init__(self, session):
        self.session = session
        self.total_processed = 0
        self.total_failed = 0

    def check_not_processed_status(self):
        """
        Check if there are records with STATUS = 'NOT PROCESSED' in the DocAI_OrderForm_Extraction table.
        """
        status_query = """
            SELECT COUNT(*) AS COUNT
            FROM DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            WHERE STATUS = 'NOT PROCESSED' AND MODEL_NAME = 'FACTORY_ORDER';
        """
        result = self.session.sql(status_query).collect()
        return result[0]['COUNT'] > 0 if result else False

    def load_thresholds(self):
        threshold_query = """
            SELECT SCORE_NAME, CAST(SCORE_VALUE AS FLOAT) AS SCORE_VALUE
            FROM DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD
            WHERE MODEL_NAME = 'FACTORY_ORDER';
        """
        thresholds = self.session.sql(threshold_query).collect()
        if not thresholds:
            raise ValueError("No thresholds found for the FACTORY_ORDER.")
        return {row['SCORE_NAME']: row['SCORE_VALUE'] for row in thresholds}

    def load_table_schema(self):
        schema_query = "DESCRIBE TABLE DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN"
        schema = self.session.sql(schema_query).collect()
        return {row['name'].upper() for row in schema}

    def insert_failed_history(self, filename, score_name, score_value, comments):
        if score_value == 0 and "missing" in comments.lower():
            print(f"Skipping history insert for missing field: {score_name}")
            return
        insert_query = f"""
            INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.ORDER_COL_SCORE_FAILED_HISTORY (
                score_name, 
                score_value, 
                date_failed, 
                filename, 
                comments
            ) VALUES (
                '{score_name.replace("'", "''")}', 
                {score_value}, 
                CURRENT_TIMESTAMP, 
                '{filename.replace("'", "''")}', 
                '{comments.replace("'", "''")}'
            );
        """
        self.session.sql(insert_query).collect()

    def insert_flatten_table(self, record, json_data, ocr_score, failed_fields, start_time, end_time):
        insert_columns = ["RELATIVEPATH", "MODEL_NAME", "OCR_SCORE"]
        insert_values = [record["RELATIVEPATH"], "FACTORY_ORDER", ocr_score]

        for field, threshold in self.thresholds.items():
            if field == "ocrScore":
                continue

            field_score_col = f"{field.upper()}_SCORE"
            field_value_col = f"{field.upper()}_VALUE"

            if field_score_col in self.valid_columns and field_value_col in self.valid_columns:
                field_data = json_data.get(field, [])
                if not field_data:
                    print(f"Field '{field}' is missing in JSON. Inserting default values.")
                    insert_columns.extend([field_score_col, field_value_col])
                    insert_values.extend([0, "NULL"])
                    continue

                score = field_data[0].get("score", 0)
                concatenated_values = ", ".join(
                    [item.get("value", "").replace("'", "''") for item in field_data]
                )
                insert_columns.extend([field_score_col, field_value_col])
                insert_values.extend([score, concatenated_values])

        status = "FAILED" if failed_fields else "PROCESSED"
        comments = f"Failed: {', '.join(failed_fields)}" if failed_fields else "All scores passed"

        insert_columns.extend(["STATUS", "COMMENTS", "PROCESSED_TIMESTAMP", "PROCESS_START_TIME", "PROCESS_END_TIME"])
        insert_values.extend([status, comments, "CURRENT_TIMESTAMP()", start_time, end_time])

        columns = ", ".join(insert_columns)
        placeholders = ", ".join(
            ["?" if val != "CURRENT_TIMESTAMP()" else "CURRENT_TIMESTAMP()" for val in insert_values]
        )
        insert_query = f"INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN ({columns}) VALUES ({placeholders})"
        self.session.sql(insert_query, [val for val in insert_values if val != "CURRENT_TIMESTAMP()"]).collect()

    def update_extraction_status(self, relative_path, status):
        update_query = f"""
            UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            SET STATUS = '{status}'
            WHERE RELATIVEPATH = '{relative_path.replace("'", "''")}';
        """
        self.session.sql(update_query).collect()

    def process_record(self, record):
        try:
            json_data = json.loads(record['JSON'])
            self.thresholds = self.load_thresholds()
            self.valid_columns = self.load_table_schema()

            process_start_time = self.session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]
            ocr_score = float(json_data.get("__documentMetadata", {}).get("ocrScore", 0))

            failed_fields = []
            if ocr_score < self.thresholds.get("ocrScore", 0):
                failed_fields.append("OCR_Score")
                self.insert_failed_history(record["RELATIVEPATH"], "OCR_Score", ocr_score, "OCR_Score failed validation")

            for field, threshold in self.thresholds.items():
                if field == "ocrScore":
                    continue
                field_data = json_data.get(field, [])
                if not field_data:
                    print(f"Field '{field}' is not present in the JSON. Skipping validation.")
                    continue
                score = field_data[0].get("score", 0)
                if score < threshold:
                    failed_fields.append(field)
                    self.insert_failed_history(
                        record["RELATIVEPATH"], field, score, f"{field} failed validation"
                    )

            process_end_time = self.session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]

            self.insert_flatten_table(record, json_data, ocr_score, failed_fields, process_start_time, process_end_time)

            self.update_extraction_status(record["RELATIVEPATH"], "PROCESSED")
            self.total_processed += 1 if not failed_fields else 0
            self.total_failed += 1 if failed_fields else 0

        except Exception as e:
            import traceback
            error_message = str(e)
            stack_trace = traceback.format_exc()
            update_comments_query = f"""
                UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
                SET STATUS = 'ERROR',
                    COMMENTS = 'Error: {error_message.replace("'", "''")} | StackTrace: {stack_trace.replace("'", "''")}'
                WHERE RELATIVEPATH = '{record["RELATIVEPATH"].replace("'", "''")}'
            """
            self.session.sql(update_comments_query).collect()
            print(f"Error processing record {record['RELATIVEPATH']}: {error_message}")
            self.total_failed += 1

    def process_all_records(self):
        records_query = """
            SELECT RELATIVEPATH, JSON
            FROM DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            WHERE STATUS = 'NOT PROCESSED' AND MODEL_NAME = 'FACTORY_ORDER';
        """
        records = self.session.sql(records_query).collect()
        if not records:
            return "No records to process."

        for record in records:
            self.process_record(record)

        return f"Processed {self.total_processed} records. Failed to process {self.total_failed} records."

def main(session: Session) -> str:
    processor = InvoiceProcessor(session)

    # Check for "NOT PROCESSED" status
    if not processor.check_not_processed_status():
        return "No records with 'NOT PROCESSED' status. Procedure terminated."

    return processor.process_all_records()
$$;



CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_FLATTEN();

select * from DOCAI_ORDERFORM_EXTRACTION;

select * from ORDER_FLATTEN;

select * from ORDER_COL_SCORE_FAILED_HISTORY;

update DOCAI_ORDERFORM_EXTRACTION set STATUS ='NOT PROCESSED' WHERE STATUS ='PROCESSED' ;

----------------------------------------------------------------------------------


CREATE OR REPLACE TABLE Delivery_Flatten (
    RELATIVEPATH STRING,                         
    MODEL_NAME STRING,                           
    OCR_SCORE FLOAT,                             
    DELIVERY_ADDRESS_Score FLOAT,
    DELIVERY_ADDRESS_Value STRING,
    SHIPPING_WEIGHT_Score FLOAT,
    SHIPPING_WEIGHT_Value STRING,
    ALTITUDE_Score FLOAT,
    ALTITUDE_Value STRING,
    AMBIENT_TEMP_Score FLOAT,
    AMBIENT_TEMP_Value STRING,
    TOTAL_KW_Score FLOAT,
    TOTAL_KW_Value STRING,
    FLOW_Score FLOAT,
    FLOW_Value STRING,
    POWER_Score FLOAT,
    POWER_Value STRING,
    EWT_Score FLOAT,
    EWT_Value STRING,
    LWT_Score FLOAT,
    LWT_Value STRING,
    EWT_F_Score FLOAT,
    EWT_F_Value STRING,
    LWT_F_Score FLOAT,
    LWT_F_Value STRING,
    PASS_Score FLOAT,
    PASS_Value STRING,
    PD_Score FLOAT,
    PD_Value STRING,
    FF_Score FLOAT,
    FF_Value STRING,
    MBH_Score FLOAT,
    MBH_Value STRING,
    COMMENTS STRING,                             
    STATUS STRING,                               
    PROCESSED_TIMESTAMP TIMESTAMP,               
    PROCESS_START_TIME TIMESTAMP,                
    PROCESS_END_TIME TIMESTAMP                   
);

CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_DELIVERY_FLATTEN()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.11'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
EXECUTE AS OWNER
AS
$$
import json
from snowflake.snowpark.session import Session

class InvoiceProcessor:

    def __init__(self, session):
        self.session = session
        self.total_processed = 0
        self.total_failed = 0

    def check_not_processed_status(self):
        """
        Check if there are records with STATUS = 'NOT PROCESSED' in the DocAI_OrderForm_Extraction table.
        """
        status_query = """
            SELECT COUNT(*) AS COUNT
            FROM DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            WHERE STATUS = 'NOT PROCESSED' AND MODEL_NAME = 'FACTORY_DELIVERY';
        """
        result = self.session.sql(status_query).collect()
        return result[0]['COUNT'] > 0 if result else False

    def load_thresholds(self):
        threshold_query = """
            SELECT SCORE_NAME, CAST(SCORE_VALUE AS FLOAT) AS SCORE_VALUE
            FROM DS_DEV_DB.DOC_AI_SCHEMA.SCORE_THRESHOLD
            WHERE MODEL_NAME = 'FACTORY_DELIVERY';
        """
        thresholds = self.session.sql(threshold_query).collect()
        if not thresholds:
            raise ValueError("No thresholds found for the FACTORY_DELIVERY.")
        return {row['SCORE_NAME']: row['SCORE_VALUE'] for row in thresholds}

    def load_table_schema(self):
        schema_query = "DESCRIBE TABLE DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_FLATTEN"
        schema = self.session.sql(schema_query).collect()
        return {row['name'].upper() for row in schema}

    def insert_failed_history(self, filename, score_name, score_value, comments):
        if score_value == 0 and "missing" in comments.lower():
            print(f"Skipping history insert for missing field: {score_name}")
            return
        insert_query = f"""
            INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.delivery_col_score_failed_history (
                score_name, 
                score_value, 
                date_failed, 
                filename, 
                comments
            ) VALUES (
                '{score_name.replace("'", "''")}', 
                {score_value}, 
                CURRENT_TIMESTAMP, 
                '{filename.replace("'", "''")}', 
                '{comments.replace("'", "''")}'
            );
        """
        self.session.sql(insert_query).collect()

    def insert_flatten_table(self, record, json_data, ocr_score, failed_fields, start_time, end_time):
        insert_columns = ["RELATIVEPATH", "MODEL_NAME", "OCR_SCORE"]
        insert_values = [record["RELATIVEPATH"], "FACTORY_DELIVERY", ocr_score]

        for field, threshold in self.thresholds.items():
            if field == "ocrScore":
                continue

            field_score_col = f"{field.upper()}_SCORE"
            field_value_col = f"{field.upper()}_VALUE"

            if field_score_col in self.valid_columns and field_value_col in self.valid_columns:
                field_data = json_data.get(field, [])
                if not field_data:
                    print(f"Field '{field}' is missing in JSON. Inserting default values.")
                    insert_columns.extend([field_score_col, field_value_col])
                    insert_values.extend([0, "NULL"])
                    continue

                score = field_data[0].get("score", 0)
                concatenated_values = ", ".join(
                    [item.get("value", "").replace("'", "''") for item in field_data]
                )
                insert_columns.extend([field_score_col, field_value_col])
                insert_values.extend([score, concatenated_values])

        status = "FAILED" if failed_fields else "PROCESSED"
        comments = f"Failed: {', '.join(failed_fields)}" if failed_fields else "All scores passed"

        insert_columns.extend(["STATUS", "COMMENTS", "PROCESSED_TIMESTAMP", "PROCESS_START_TIME", "PROCESS_END_TIME"])
        insert_values.extend([status, comments, "CURRENT_TIMESTAMP()", start_time, end_time])

        columns = ", ".join(insert_columns)
        placeholders = ", ".join(
            ["?" if val != "CURRENT_TIMESTAMP()" else "CURRENT_TIMESTAMP()" for val in insert_values]
        )
        insert_query = f"INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_FLATTEN ({columns}) VALUES ({placeholders})"
        self.session.sql(insert_query, [val for val in insert_values if val != "CURRENT_TIMESTAMP()"]).collect()

    def update_extraction_status(self, relative_path, status):
        update_query = f"""
            UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            SET STATUS = '{status}'
            WHERE RELATIVEPATH = '{relative_path.replace("'", "''")}';
        """
        self.session.sql(update_query).collect()

    def process_record(self, record):
        try:
            json_data = json.loads(record['JSON'])
            self.thresholds = self.load_thresholds()
            self.valid_columns = self.load_table_schema()

            process_start_time = self.session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]
            ocr_score = float(json_data.get("__documentMetadata", {}).get("ocrScore", 0))

            failed_fields = []
            if ocr_score < self.thresholds.get("ocrScore", 0):
                failed_fields.append("OCR_Score")
                self.insert_failed_history(record["RELATIVEPATH"], "OCR_Score", ocr_score, "OCR_Score failed validation")

            for field, threshold in self.thresholds.items():
                if field == "ocrScore":
                    continue
                field_data = json_data.get(field, [])
                if not field_data:
                    print(f"Field '{field}' is not present in the JSON. Skipping validation.")
                    continue
                score = field_data[0].get("score", 0)
                if score < threshold:
                    failed_fields.append(field)
                    self.insert_failed_history(
                        record["RELATIVEPATH"], field, score, f"{field} failed validation"
                    )

            process_end_time = self.session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]

            self.insert_flatten_table(record, json_data, ocr_score, failed_fields, process_start_time, process_end_time)

            self.update_extraction_status(record["RELATIVEPATH"], "PROCESSED")
            self.total_processed += 1 if not failed_fields else 0
            self.total_failed += 1 if failed_fields else 0

        except Exception as e:
            import traceback
            error_message = str(e)
            stack_trace = traceback.format_exc()
            update_comments_query = f"""
                UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
                SET STATUS = 'ERROR',
                    COMMENTS = 'Error: {error_message.replace("'", "''")} | StackTrace: {stack_trace.replace("'", "''")}'
                WHERE RELATIVEPATH = '{record["RELATIVEPATH"].replace("'", "''")}'
            """
            self.session.sql(update_comments_query).collect()
            print(f"Error processing record {record['RELATIVEPATH']}: {error_message}")
            self.total_failed += 1

    def process_all_records(self):
        records_query = """
            SELECT RELATIVEPATH, JSON
            FROM DS_DEV_DB.DOC_AI_SCHEMA.DocAI_OrderForm_Extraction
            WHERE STATUS = 'NOT PROCESSED' AND MODEL_NAME = 'FACTORY_DELIVERY';
        """
        records = self.session.sql(records_query).collect()
        if not records:
            return "No records to process."

        for record in records:
            self.process_record(record)

        return f"Processed {self.total_processed} records. Failed to process {self.total_failed} records."

def main(session: Session) -> str:
    processor = InvoiceProcessor(session)

    # Check for "NOT PROCESSED" status
    if not processor.check_not_processed_status():
        return "No records with 'NOT PROCESSED' status. Procedure terminated."

    return processor.process_all_records()
$$;


call LOAD_DELIVERY_FLATTEN();

select * from DOCAI_ORDERFORM_EXTRACTION;

select * from DELIVERY_FLATTEN;

select * from DELIVERY_COL_SCORE_FAILED_HISTORY;

update DOCAI_ORDERFORM_EXTRACTION set STATUS ='NOT PROCESSED' WHERE STATUS ='PROCESSED' ;

----------------------------------------------------------------------------------


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_FLATTEN()
RETURNS VARCHAR(16777216)
LANGUAGE SQL
EXECUTE AS OWNER
AS
$$
BEGIN
    -- Call the first procedure
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_FLATTEN();

    -- Call the second procedure
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_DELIVERY_FLATTEN();

    -- Return a success message
    RETURN 'All procedures executed successfully.';
END;
$$;


CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_FLATTEN();

-------------------------------------------------------------------------------------


CREATE OR REPLACE TABLE Order_Validated (
    RELATIVEPATH STRING,
    Contract_Order_Number_Value STRING,
    Order_Placed_Date_Value STRING,
    Revision_Number_Value STRING,
    Liquidated_Damages_Value STRING,
    Order_Details_Description_Value STRING,
    Fabrication_Status_Value STRING,
    Revision_Date_Value STRING,
    Liquidated_Damages_Comment_Value STRING,
    Contract_Order_No_Value STRING,
    Order_Rev_No_Value STRING,
    Unit_Tag_Value STRING,
    Model_No_Value STRING,
    Capacity_Value STRING,
    Refrigerant_Value STRING,
    PIN_Value STRING,
    EWT_Value STRING,
    EWT_F_Value STRING,
    LWT_Value STRING,
    LWT_F_Value STRING,
    Flow_Value STRING,
    PD_Value STRING,
    Fluid_Value STRING,
    FF_Value STRING,
    MBH_Value STRING,
    Pass_Value STRING,
    Power_Value STRING,
    Total_KW_Value STRING,
    AmbientTemp_Value STRING,
    Altitude_Value STRING,
    Type_Starter_Value STRING,
    Shipping_Weight_Value STRING,
    Special_Quotes_Value STRING,
    PROCESS_START_TIME TIMESTAMP,
    PROCESS_END_TIME TIMESTAMP
);


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_VALIDATED()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
EXECUTE AS OWNER
AS
$$
def main(session):
    try:
        total_processed = 0

        # Fetch all processed records from ORDER_FLATTEN
        select_query = """
            SELECT 
                RELATIVEPATH,
            	CONTRACT_ORDER_NUMBER_VALUE,
            	ORDER_PLACED_DATE_VALUE,
            	REVISION_NUMBER_VALUE,
            	LIQUIDATED_DAMAGES_VALUE,
            	ORDER_DETAILS_DESCRIPTION_VALUE,
            	FABRICATION_STATUS_VALUE,
            	REVISION_DATE_VALUE,
            	LIQUIDATED_DAMAGES_COMMENT_VALUE,
            	CONTRACT_ORDER_NO_VALUE,
            	ORDER_REV_NO_VALUE,
            	MODEL_NO_VALUE,
            	CAPACITY_VALUE,
            	REFRIGERANT_VALUE,
            	PIN_VALUE,
            	EWT_VALUE,
                EWT_F_VALUE,
            	LWT_VALUE,
                LWT_F_VALUE,
            	FLOW_VALUE,
            	PD_VALUE,
            	FLUID_VALUE,
            	FF_VALUE,
            	MBH_VALUE,
            	PASS_VALUE,
            	POWER_VALUE,
            	TOTAL_KW_VALUE,
            	AMBIENTTEMP_VALUE,
            	ALTITUDE_VALUE,
            	TYPE_STARTER_VALUE,
            	SHIPPING_WEIGHT_VALUE,
            	SPECIAL_QUOTES_VALUE,
                PROCESS_START_TIME,
                PROCESS_END_TIME
            FROM DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN
            WHERE STATUS = 'PROCESSED'
        """
        rows_to_insert = session.sql(select_query).collect()

        # If no records to process, exit
        if not rows_to_insert:
            return "No records to process from ORDER_FLATTEN."

        print(f"Found {len(rows_to_insert)} records to process.")

        for record in rows_to_insert:
            try:
                # Insert into ORDER_VALIDATED
                insert_query = f"""
                    INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.ORDER_VALIDATED (
                        RELATIVEPATH,
                    	CONTRACT_ORDER_NUMBER_VALUE,
                    	ORDER_PLACED_DATE_VALUE,
                    	REVISION_NUMBER_VALUE,
                    	LIQUIDATED_DAMAGES_VALUE,
                    	ORDER_DETAILS_DESCRIPTION_VALUE,
                    	FABRICATION_STATUS_VALUE,
                    	REVISION_DATE_VALUE,
                    	LIQUIDATED_DAMAGES_COMMENT_VALUE,
                    	CONTRACT_ORDER_NO_VALUE,
                    	ORDER_REV_NO_VALUE,
                    	MODEL_NO_VALUE,
                    	CAPACITY_VALUE,
                    	REFRIGERANT_VALUE,
                    	PIN_VALUE,
                    	EWT_VALUE,
                        EWT_F_VALUE,
                    	LWT_VALUE,
                        LWT_F_VALUE,
                    	FLOW_VALUE,
                    	PD_VALUE,
                    	FLUID_VALUE,
                    	FF_VALUE,
                    	MBH_VALUE,
                    	PASS_VALUE,
                    	POWER_VALUE,
                    	TOTAL_KW_VALUE,
                    	AMBIENTTEMP_VALUE,
                    	ALTITUDE_VALUE,
                    	TYPE_STARTER_VALUE,
                    	SHIPPING_WEIGHT_VALUE,
                    	SPECIAL_QUOTES_VALUE,
                        PROCESS_START_TIME,
                        PROCESS_END_TIME
                    ) VALUES (
                        '{record["RELATIVEPATH"].replace("'", "''")}',
                        '{record["CONTRACT_ORDER_NUMBER_VALUE"].replace("'", "''") if record["CONTRACT_ORDER_NUMBER_VALUE"] else ""}',
                        '{record["ORDER_PLACED_DATE_VALUE"].replace("'", "''") if record["ORDER_PLACED_DATE_VALUE"] else ""}',
                        '{record["REVISION_NUMBER_VALUE"].replace("'", "''") if record["REVISION_NUMBER_VALUE"] else ""}',
                        '{record["LIQUIDATED_DAMAGES_VALUE"].replace("'", "''") if record["LIQUIDATED_DAMAGES_VALUE"] else ""}',
                        '{record["ORDER_DETAILS_DESCRIPTION_VALUE"].replace("'", "''") if record["ORDER_DETAILS_DESCRIPTION_VALUE"] else ""}',
                        '{record["FABRICATION_STATUS_VALUE"].replace("'", "''") if record["FABRICATION_STATUS_VALUE"] else ""}',

                        '{record["REVISION_DATE_VALUE"].replace("'", "''") if record["REVISION_DATE_VALUE"] else ""}',
                        '{record["LIQUIDATED_DAMAGES_COMMENT_VALUE"].replace("'", "''") if record["LIQUIDATED_DAMAGES_COMMENT_VALUE"] else ""}',
                        '{record["CONTRACT_ORDER_NO_VALUE"].replace("'", "''") if record["CONTRACT_ORDER_NO_VALUE"] else ""}',
                        '{record["ORDER_REV_NO_VALUE"].replace("'", "''") if record["ORDER_REV_NO_VALUE"] else ""}',
                        '{record["MODEL_NO_VALUE"].replace("'", "''") if record["MODEL_NO_VALUE"] else ""}',
                        '{record["CAPACITY_VALUE"].replace("'", "''") if record["CAPACITY_VALUE"] else ""}',

                        '{record["REFRIGERANT_VALUE"].replace("'", "''") if record["REFRIGERANT_VALUE"] else ""}',
                        '{record["PIN_VALUE"].replace("'", "''") if record["PIN_VALUE"] else ""}',
                        '{record["EWT_VALUE"].replace("'", "''") if record["EWT_VALUE"] else ""}',
                        '{record["EWT_F_VALUE"].replace("'", "''") if record["EWT_F_VALUE"] else ""}',
                        '{record["LWT_VALUE"].replace("'", "''") if record["LWT_VALUE"] else ""}',
                        '{record["LWT_F_VALUE"].replace("'", "''") if record["LWT_F_VALUE"] else ""}',
                        '{record["FLOW_VALUE"].replace("'", "''") if record["FLOW_VALUE"] else ""}',
                        '{record["PD_VALUE"].replace("'", "''") if record["PD_VALUE"] else ""}',

                        '{record["FLUID_VALUE"].replace("'", "''") if record["FLUID_VALUE"] else ""}',
                        '{record["FF_VALUE"].replace("'", "''") if record["FF_VALUE"] else ""}',
                        '{record["MBH_VALUE"].replace("'", "''") if record["MBH_VALUE"] else ""}',
                        '{record["PASS_VALUE"].replace("'", "''") if record["PASS_VALUE"] else ""}',
                        '{record["POWER_VALUE"].replace("'", "''") if record["POWER_VALUE"] else ""}',
                        '{record["TOTAL_KW_VALUE"].replace("'", "''") if record["TOTAL_KW_VALUE"] else ""}',

                        '{record["AMBIENTTEMP_VALUE"].replace("'", "''") if record["AMBIENTTEMP_VALUE"] else ""}',
                        '{record["ALTITUDE_VALUE"].replace("'", "''") if record["ALTITUDE_VALUE"] else ""}',
                        '{record["TYPE_STARTER_VALUE"].replace("'", "''") if record["TYPE_STARTER_VALUE"] else ""}',
                        '{record["SHIPPING_WEIGHT_VALUE"].replace("'", "''") if record["SHIPPING_WEIGHT_VALUE"] else ""}',
                        '{record["SPECIAL_QUOTES_VALUE"].replace("'", "''") if record["SPECIAL_QUOTES_VALUE"] else ""}',
                        
                        '{record["PROCESS_START_TIME"]}',
                        '{record["PROCESS_END_TIME"]}'
                    )
                """
                session.sql(insert_query).collect()

                # Update the record in ORDER_FLATTEN
                update_query = f"""
                    UPDATE DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN
                    SET STATUS = 'VALIDATED'
                    WHERE RELATIVEPATH = '{record["RELATIVEPATH"].replace("'", "''")}'
                """
                session.sql(update_query).collect()

                total_processed += 1

            except Exception as e:
                # Handle errors and update the record status
                error_message = f"Error processing record {record['RELATIVEPATH']}: {str(e)}"
                print(error_message)

                update_error_query = f"""
                    UPDATE DS_DEV_DB.DOC_AI_SCHEMA.ORDER_FLATTEN
                    SET STATUS = 'ERROR',
                        COMMENTS = '{error_message.replace("'", "''")}'
                    WHERE RELATIVEPATH = '{record["RELATIVEPATH"].replace("'", "''")}'
                """
                session.sql(update_error_query).collect()

        return f"Successfully validated {total_processed} records."

    except Exception as e:
        return f"Procedure failed: {str(e)}"
$$;

CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_VALIDATED();

select * from ORDER_FLATTEN;

update ORDER_FLATTEN set STATUS ='PROCESSED' WHERE STATUS ='ERROR';

select * from ORDER_VALIDATED;

-------------------------------------------------------------------------------------

CREATE OR REPLACE TABLE Delivery_Validated (
    RELATIVEPATH STRING,                    
    DELIVERY_ADDRESS_Value STRING,
    SHIPPING_WEIGHT_Value STRING,
    ALTITUDE_Value STRING,
    AMBIENT_TEMP_Value STRING,
    TOTAL_KW_Value STRING,
    FLOW_Value STRING,
    POWER_Value STRING,
    EWT_Value STRING,
    LWT_Value STRING,
    EWT_F_Value STRING,
    LWT_F_Value STRING,
    PASS_Value STRING,
    PD_Value STRING,
    FF_Value STRING,
    MBH_Value STRING,
    PROCESS_START_TIME TIMESTAMP,           
    PROCESS_END_TIME TIMESTAMP              
);

CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_DELIVERY_VALIDATED()
RETURNS VARCHAR(16777216)
LANGUAGE PYTHON
RUNTIME_VERSION = '3.8'
PACKAGES = ('snowflake-snowpark-python')
HANDLER = 'main'
EXECUTE AS OWNER
AS
$$
def main(session):
    try:
        total_processed = 0

        # Fetch all processed records from DELIVERY_FLATTEN
        select_query = """
            SELECT 
                RELATIVEPATH,
                DELIVERY_ADDRESS_VALUE,
            	SHIPPING_WEIGHT_VALUE,
            	ALTITUDE_VALUE,
            	AMBIENT_TEMP_VALUE,
            	TOTAL_KW_VALUE,
            	FLOW_VALUE,
            	POWER_VALUE,
            	EWT_VALUE,
            	LWT_VALUE,
            	EWT_F_VALUE,
            	LWT_F_VALUE,
            	PASS_VALUE,
            	PD_VALUE,
            	FF_VALUE,
            	MBH_VALUE
            FROM DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_FLATTEN
            WHERE STATUS = 'PROCESSED'
            ORDER BY RELATIVEPATH;
        """
        rows_to_insert = session.sql(select_query).collect()

        # If no records to process, exit
        if not rows_to_insert:
            return "No records to process from DELIVERY_FLATTEN."

        print(f"Found {len(rows_to_insert)} records to process.")

        for record in rows_to_insert:
            try:
                # Start processing time
                process_start_time = session.sql("SELECT CURRENT_TIMESTAMP").collect()[0][0]

                # Helper function to handle NULL values
                def handle_null(value):
                    return value.replace("'", "''") if value else ""

                # Insert into DELIVERY_VALIDATED
                insert_query = f"""
                    INSERT INTO DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_VALIDATED (
                        RELATIVEPATH,
                        DELIVERY_ADDRESS_VALUE,
                    	SHIPPING_WEIGHT_VALUE,
                    	ALTITUDE_VALUE,
                    	AMBIENT_TEMP_VALUE,
                    	TOTAL_KW_VALUE,
                    	FLOW_VALUE,
                    	POWER_VALUE,
                    	EWT_VALUE,
                    	LWT_VALUE,
                    	EWT_F_VALUE,
                    	LWT_F_VALUE,
                    	PASS_VALUE,
                    	PD_VALUE,
                    	FF_VALUE,
                    	MBH_VALUE,
                        PROCESS_START_TIME,
                        PROCESS_END_TIME
                    ) VALUES (
                        '{handle_null(record["RELATIVEPATH"])}',
                        '{handle_null(record["DELIVERY_ADDRESS_VALUE"])}',
                        '{handle_null(record["SHIPPING_WEIGHT_VALUE"])}',
                        '{handle_null(record["ALTITUDE_VALUE"])}',
                        '{handle_null(record["AMBIENT_TEMP_VALUE"])}',
                        '{handle_null(record["TOTAL_KW_VALUE"])}',
                        '{handle_null(record["FLOW_VALUE"])}',
                        '{handle_null(record["POWER_VALUE"])}',

                        '{handle_null(record["EWT_VALUE"])}',
                        '{handle_null(record["LWT_VALUE"])}',
                        '{handle_null(record["EWT_F_VALUE"])}',
                        '{handle_null(record["LWT_F_VALUE"])}',
                        '{handle_null(record["PASS_VALUE"])}',
                        '{handle_null(record["PD_VALUE"])}',
                        '{handle_null(record["FF_VALUE"])}',
                        '{handle_null(record["MBH_VALUE"])}',
                        '{process_start_time}',
                        CURRENT_TIMESTAMP
                    )
                """
                session.sql(insert_query).collect()

                # Update the record in DELIVERY_FLATTEN
                update_query = f"""
                    UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_FLATTEN
                    SET STATUS = 'VALIDATED'
                    WHERE RELATIVEPATH = '{handle_null(record["RELATIVEPATH"])}';
                """
                session.sql(update_query).collect()

                total_processed += 1

            except Exception as e:
                # Handle errors and update the record status
                error_message = f"Error processing record {record['RELATIVEPATH']}: {str(e)}"
                print(error_message)

                update_error_query = f"""
                    UPDATE DS_DEV_DB.DOC_AI_SCHEMA.DELIVERY_FLATTEN
                    SET STATUS = 'ERROR',
                        COMMENTS = '{error_message.replace("'", "''")}'
                    WHERE RELATIVEPATH = '{handle_null(record["RELATIVEPATH"])}';
                """
                session.sql(update_error_query).collect()

        return f"Successfully validated {total_processed} records."

    except Exception as e:
        return f"Procedure failed: {str(e)}"
$$;


call LOAD_DELIVERY_VALIDATED();

select * from DELIVERY_FLATTEN;

select * from DELIVERY_VALIDATED;

----------------------------------------------------------------------------------------------------------


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_VALIDATED()
RETURNS VARCHAR(16777216)
LANGUAGE SQL
EXECUTE AS OWNER
AS
$$
BEGIN
    -- Call the first procedure
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ORDERFORM_VALIDATED();

    -- Call the second procedure
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_DELIVERY_VALIDATED();

    -- Return a success message
    RETURN 'All procedures executed successfully.';
END;
$$;


CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_VALIDATED();

-------------------------------------------------------------------------------------------------------------
CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.MOVEFILES_TO_SOURCE(selected_file VARCHAR)
RETURNS VARCHAR(16777216)
LANGUAGE JAVASCRIPT
EXECUTE AS CALLER
AS '
try {
    var selectedFile = arguments[0];
    var escapedFileName = selectedFile.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
    var completePattern = `.*${escapedFileName}.*`;

    var list_files_query = `LIST @manual_review PATTERN = ''${completePattern}''`;
    var files_list = snowflake.execute({ sqlText: list_files_query });

    var subfolder = null;
    while (files_list.next()) {
        var full_path = files_list.getColumnValue(1);
        var parts = full_path.split("/");
        if (parts.length > 1) {
            subfolder = parts[1]; // Extract the immediate subfolder
            break;
        }
    }

    if (!subfolder || (subfolder !== "Order" && subfolder !== "Delivery")) {
        throw `Invalid or missing subfolder for file "${selectedFile}". Allowed folders are "Order" and "Delivery".`;
    }

    var copy_file_query = `
        COPY FILES INTO @DOC_STAGE
        FROM @manual_review
        PATTERN = ''${completePattern}''`;
    snowflake.execute({ sqlText: copy_file_query });

    var remove_file_query = `
        REMOVE @manual_review
        PATTERN = ''${completePattern}''`;
    snowflake.execute({ sqlText: remove_file_query });

    return `File "${selectedFile}" successfully moved from manual_review to DOC_STAGE/${subfolder}.`;
} catch (err) {
    return `Failed to process file "${selectedFile}": ${err}`;
}
';



CALL DS_DEV_DB.DOC_AI_SCHEMA.MOVEFILES_TO_SOURCE('1N830127-005.pdf');


CREATE OR REPLACE PROCEDURE DS_DEV_DB.DOC_AI_SCHEMA.IGNOREFILES_TO_STAGE(selected_file VARCHAR)
RETURNS VARCHAR(16777216)
LANGUAGE JAVASCRIPT
EXECUTE AS CALLER
AS '
try {
    // Fetch the selected_file parameter
    var selectedFile = arguments[0];

    // Escape special characters in the filename for regex
    var escapedFileName = selectedFile.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");

    // Create a regex pattern for the file
    var completePattern = `.*${escapedFileName}.*`;

    // List files to identify the subfolder
    var list_files_query = `
        LIST @manual_review PATTERN = ''${completePattern}''`;
    var files_list = snowflake.execute({ sqlText: list_files_query });

    var subfolder = null;
    while (files_list.next()) {
        var full_path = files_list.getColumnValue(1); // Get the full path of the file

        // Extract subfolder from the path
        var parts = full_path.split("/"); // Split the path into parts
        if (parts.length > 1) {
            subfolder = parts[1]; // Extract the subfolder name 
            break; // Exit loop once subfolder is identified
        }
    }

    // If no subfolder is identified, throw an error
    if (!subfolder) {
        throw `Subfolder not identified for file "${selectedFile}".`;
    }

    // Correctly rename the subfolder to "removed_<subfolder>"
    var renamedSubfolder = `removed_${subfolder}`;

    // Copy the file from manual_review stage to the renamed subfolder in IGNOIRED_DOCS stage
    var copy_file_query = `
        COPY FILES INTO @IGNOIRED_DOCS
        FROM @manual_review
        PATTERN = ''${completePattern}''`;
    snowflake.execute({ sqlText: copy_file_query });

    // Remove the file from manual_review stage
    var remove_file_query = `
        REMOVE @manual_review
        PATTERN = ''${completePattern}''`;
    snowflake.execute({ sqlText: remove_file_query });

    // Return success message
    return `File "${selectedFile}" successfully moved from manual_review to IGNOIRED_DOCS/${renamedSubfolder} stage.`;
} catch (err) {
    return `Failed to process file "${selectedFile}": ${err}`;
}
';


CALL DS_DEV_DB.DOC_AI_SCHEMA.IGNOREFILES_TO_STAGE('1J080798-001.pdf');

select * from manual_review_history_log;

-------------------------------------------------------------------------------------



create or replace task DS_DEV_DB.DOC_AI_SCHEMA.PREPROCESSING
	warehouse=DS_DEV_WH
	schedule='1 MINUTE'
	when SYSTEM$STREAM_HAS_DATA('PREPROCESS_STREAM')
	as BEGIN
    CALL COUNT_PDF_PAGES_PROC();
END;

-------------
create or replace task DS_DEV_DB.DOC_AI_SCHEMA.EXTRACT_DATA
	warehouse=DS_DEV_WH
	schedule='1 MINUTE'
	as BEGIN
    CALL Handle_PDF_Files();
END;

------------------
CREATE OR REPLACE TASK DS_DEV_DB.DOC_AI_SCHEMA.MANUAL_CHECKUP_FILES
WAREHOUSE = DS_DEV_WH
AFTER DS_DEV_DB.DOC_AI_SCHEMA.EXTRACT_DATA
AS
BEGIN
    -- Call procedures for both models
    CALL DS_DEV_DB.DOC_AI_SCHEMA.MANAGE_MANUAL_REVIEW_FILES();
END;

----------------------
CREATE OR REPLACE TASK DS_DEV_DB.DOC_AI_SCHEMA.FLATTEN_DATA
WAREHOUSE = DS_DEV_WH
AFTER DS_DEV_DB.DOC_AI_SCHEMA.EXTRACT_DATA
AS
BEGIN
    -- Call procedures for both models
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_FLATTEN();
END;

-------------------
CREATE OR REPLACE TASK DS_DEV_DB.DOC_AI_SCHEMA.VALIDATE_DATA
WAREHOUSE = DS_DEV_WH
AFTER DS_DEV_DB.DOC_AI_SCHEMA.FLATTEN_DATA
AS
BEGIN
    -- Call procedures for both models
    CALL DS_DEV_DB.DOC_AI_SCHEMA.LOAD_ALL_VALIDATED();
END;

------------------------------------------------------------------------------

